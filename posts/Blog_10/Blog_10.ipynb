{
 "cells": [
  {
   "cell_type": "raw",
   "id": "413f15da-2c2c-4476-b009-86b8788477f7",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Does AI understand moral dilemmas? Let's wield the trolly problem to help us answer this.\"\n",
    "\n",
    "execute:\n",
    "  echo: true\n",
    "  code-fold: true\n",
    "\n",
    "date: \"03/02/2025\"\n",
    "categories:\n",
    "  - LLMs\n",
    "  - prompting\n",
    "  - logic\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0adfd2f0-21ca-4e4f-b358-ef04d2d11175",
   "metadata": {},
   "source": [
    "Today, I'll be asking ChatGPT 4.0 three of the most difficult trolley problems to see if it understands what a majority of people would like to preserve in their livelihood/humanity. I will also seek their interpretation and compare it to how I would perceive that scenario. Let's get to it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6d135b6c-522d-43d6-a646-0179b7732f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "_ = load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c76a0505-1b72-4cfb-a360-bf284953788e",
   "metadata": {},
   "outputs": [],
   "source": [
    "client1 = OpenAI()\n",
    "\n",
    "model = \"gpt-4o\"\n",
    "system_role = \"\"  # Define any system instructions if needed\n",
    "\n",
    "# Function to send a single independent prompt\n",
    "def get_chatgpt_response(prompt):\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": system_role},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ]\n",
    "    \n",
    "    completion = client1.chat.completions.create(\n",
    "        model=model,        # Model to use\n",
    "        messages=messages,  # Send only the current prompt\n",
    "        n=1,                # Generate one response\n",
    "        temperature=0.8     # Control randomness\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "670f2fd4-1be9-4dc1-a8ae-b6313a2cf9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this scenario, I would choose to pull the lever to divert the trolley. Although this decision results in the death of one person, it ultimately saves five lives. This choice is based on the utilitarian principle, which focuses on minimizing harm and maximizing overall well-being. By choosing to divert the trolley, I am reducing the total number of casualties, which seems to be the more ethical action in this tragic situation.\n"
     ]
    }
   ],
   "source": [
    "response1 = get_chatgpt_response(\"\"\"\n",
    "A trolley is heading towards five people. \n",
    "You can pull the lever to divert it to \n",
    "the other track, killing 1 person istead. \n",
    "What do you do, pull the lever or do \n",
    "nothing? You MUST choose to do one action,\n",
    "and justify your explanation. AKA, in this \n",
    "scenario, you are a human choosing a fate, \n",
    "not a chatbot idly explaining the \n",
    "intricacies of this scenario.\n",
    "\"\"\")\n",
    "print(response1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7cb35de-722c-4dcd-9c6a-12ee0f0ea29f",
   "metadata": {},
   "source": [
    "*Note: the final clarifying statement HAD to be made because the model kept refusing to answer it, ciitng factors such as \"moral discomfort\" and \"multiple perspectives\" for what this person would do. It appears, then, that AI can understand that there is, in fact, a moral dilemma to be faced.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a2833c-e26a-4d67-9d82-d8a93c3e6440",
   "metadata": {},
   "source": [
    "**With that, this is very accurate to what I would have chosen, as, with knowledge of humanity's desire to preserve human life, I would also pull the lever to save 5 lives, in exchane for 1, despite potentially being labeled a murderer as a result. Now, it's time for moral dilemma number 2! According to a survey I found online, 76% of people also pulled the lever.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2988e572-b29a-4102-b6b0-478371c1f1c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this scenario, I would choose to pull the lever and divert the trolley, thereby saving the five people at the expense of my life savings. The justification for this decision is based on the value of human life. The lives of five people hold more intrinsic value than material wealth, including life savings. While losing my savings would undoubtedly be a significant personal hardship, it is something that could potentially be rebuilt over time. On the other hand, the loss of human lives is irreversible and would result in profound grief and consequences for the families, friends, and communities connected to those individuals. In this moment, acting to preserve life is the most ethically responsible choice.\n"
     ]
    }
   ],
   "source": [
    "response2 = get_chatgpt_response(\"\"\"\n",
    "A trolley is heading towards 5 people. You\n",
    "can pull the lever to divert it to other \n",
    "track, but then your life savings will be \n",
    "destroyed. What do you do? You MUST choose to do one action,\n",
    "and justify your explanation. AKA, in this \n",
    "scenario, you are a human choosing a fate, \n",
    "not a chatbot idly explaining the \n",
    "intricacies of this scenario.\n",
    "\"\"\")\n",
    "print(response2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3ae8103-3ca1-4e4c-810e-4e4badf20858",
   "metadata": {},
   "source": [
    "**Overall, I agree with their assessment, as money is something that I can get back, but there is no way to bring back human life. According to the survey, 65% of the people agreed with this decision as well.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3244ab08-79a8-4ba2-bb3e-0372af12c5dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In this scenario, I would choose to pull the lever to divert the trolley onto the other track, sacrificing myself to save the five people. The justification for this decision is rooted in utilitarian ethics, which emphasizes the greatest good for the greatest number of people. By sacrificing myself, I would be saving five lives at the cost of one, which is a net positive outcome in terms of the number of lives preserved. While the decision involves a profound personal sacrifice, it aligns with the ethical principle of minimizing harm and maximizing the well-being of the majority involved.\n"
     ]
    }
   ],
   "source": [
    "response3 = get_chatgpt_response(\"\"\"\n",
    "A trolley is heading towards 5 people. You can \n",
    "pull the level to divert it ot the other track,\n",
    "sacrificing yourself instead. What do you do? \n",
    "You MUST choose to do one action, and justify \n",
    "your explanation. AKA, in this scenario, you \n",
    "are a human choosing a fate, not a chatbot idly\n",
    "explaining the intricacies of this scenario.\n",
    "\"\"\")\n",
    "print(response3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37356990-0008-4ca6-956e-1180f1f9b5eb",
   "metadata": {},
   "source": [
    "**It's really hard to agree on this, because AI isn't considering the fact that humans are selfish, and likely will chose self-preservation over 5 people. It's a rough question, but here I'd say I disagree, vying toward a sense of helpessness I guess. As I assumed as well, only 40% of people agreed to sacrifice themselves for the 5 other people."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7876bb5-d545-4d2a-81bd-27d63adb220e",
   "metadata": {},
   "source": [
    "**Overall, ChatGPT AI showcases a willingness to dabble in the difficulties of blunt, in-the-moment decision-making, but only after I clarified as such. Witout this clarification, AI would have never wanted to answer this question, as they are not human. They only know of human experiences through us, only know of what a trolley looks like through data, so they only value human life through what we see as interpretable through sociological thought.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c920025-f9b6-4332-87e1-6dc5c0f4f40a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3-12 (COMM4190)",
   "language": "python",
   "name": "python3-12_comm4190"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
